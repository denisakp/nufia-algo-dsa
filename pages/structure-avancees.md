# Programmation Dynamique - Ajouter une Structure Cache

## Principe Fondamental:
- **ProblÃ¨me:** Recalculs rÃ©pÃ©titifs coÃ»teux
- **Solution:** Ajouter une structure de mÃ©morisation
- **RÃ©sultat:** ComplexitÃ© exponentielle â†’ polynomiale

<br>

## Exemples Classiques:
- **Fibonacci:** Dict pour stocker les rÃ©sultats calculÃ©s
- **Sac Ã  dos:** Tableau 2D pour les sous-problÃ¨mes
- **Plus court chemin:** Matrice des distances

<br>

ğŸ’¡ **Pattern:** Structure de cache transforme la rÃ©cursion naÃ¯ve en solution efficace !

ğŸ’¾ **Ajouter une structure de stockage = transformer un problÃ¨me difficile en facile**

<!--
ğŸ¯ Objectif du slide : Montrer que la structure cachÃ©e permet de transformer un problÃ¨me inefficace en solution optimisÃ©e.

La programmation dynamique est une technique trÃ¨s puissanteâ€¦ mais en rÃ©alitÃ©, câ€™est souvent juste de la rÃ©cursion + un cache.
	
Le problÃ¨me initial ? On recalcule les mÃªmes choses encore et encore.
	
    Exemple : dans Fibonacci (modÃ©lise une croissance cumulative, oÃ¹ le passÃ© immÃ©diat influence directement le prÃ©sent, avec une structure rÃ©cursive naturelle) naÃ¯f (ignore quâ€™il lâ€™a dÃ©jÃ  fait.), f(30) appelle f(29) et f(28), mais f(28) est recalculÃ© plusieurs fois.
	
    La solution : mÃ©moriser ce quâ€™on a dÃ©jÃ  fait.
	
    On ajoute une structure (comme un tableau ou un dictionnaire), et dÃ¨s quâ€™un sous-problÃ¨me est rÃ©solu, on stocke la rÃ©ponse.
	
    RÃ©sultat ? On passe de complexitÃ© exponentielle Ã  polynomiale.
	
Cette idÃ©e sâ€™applique partout :
	
    - Fibonacci â†’ dict simple

    - ProblÃ¨me du sac Ã  dos â†’ tableau 2D

    - Plus court chemin (Floyd-Warshall) â†’ matrice de distances
	
Conclusion Ã  marteler : ajouter une structure mÃ©moire transforme la complexitÃ© et rend les problÃ¨mes gÃ©rables !
-->

---

# Structures Probabilistes - Approximation StructurÃ©e

## Bloom Filters:
- **Principe:** Structure compacte pour rÃ©ponses probabilistes
- **Garantie:** "Probablement prÃ©sent" ou "Certainement absent"
- **Avantage:** MÃ©moire constante, vitesse O(1)

<br>

## Applications RÃ©elles:
- **Navigateurs web:** URLs malveillantes
- **Bases de donnÃ©es:** Ã‰viter les accÃ¨s disque inutiles
- **CDN:** Cache distribuÃ©
- **Bitcoin:** VÃ©rification rapide des transactions

<br>

ğŸ² **Parfois, une approximation structurÃ©e est plus utile qu'une certitude coÃ»teuse**

<!--
ğŸ¯ Objectif du slide : PrÃ©senter des structures comme les Bloom filters et expliquer pourquoi les approximations sont parfois plus utiles que la prÃ©cision.

Ici, on entre dans un monde intÃ©ressant : celui des structures â€œimprÃ©cisesâ€â€¦ mais trÃ¨s utiles.

Le Bloom filter est une structure qui peut vous dire :
â€œCet Ã©lÃ©ment est probablement prÃ©sentâ€ ou â€œsÃ»rement absentâ€

Ã‡a paraÃ®t bizarre ? Mais câ€™est extrÃªmement utile quand :
- La mÃ©moire est limitÃ©e
- La vitesse est critique

Exemple concret :
- Dans les navigateurs, les URLs malveillantes sont vÃ©rifiÃ©es avec des Bloom filters.
- En base de donnÃ©es, on Ã©vite des accÃ¨s disque coÃ»teux inutilement.

Attention : on peut avoir de faux positifs, mais jamais de faux nÃ©gatifs.

Le trade-off est clair : vous gagnez Ã©normÃ©ment en performance, pour un risque minime dâ€™erreur.

Message Ã  transmettre : parfois, une bonne approximation vaut mieux quâ€™une certitude coÃ»teuse.
-->

---

# Structures Persistantes - ImmutabilitÃ© StructurÃ©e

## Concept:
- **ImmutabilitÃ©:** Les donnÃ©es ne changent jamais
- **Nouvelles versions:** CrÃ©ation de nouvelles structures
- **Partage:** RÃ©utilisation des parties communes

<br>

## Avantages Structurels:
- **ParallÃ©lisme:** Pas de verrous nÃ©cessaires
- **Historique:** Toutes les versions conservÃ©es
- **Debugging:** Ã‰tats prÃ©cÃ©dents accessibles
- **Undo/Redo:** Naturellement implÃ©mentÃ©

<br>

ğŸ”’ **Structure immuable = garanties de cohÃ©rence et parallÃ©lisme naturel**

<!--
ğŸ¯ Objectif du slide : Montrer les avantages structurels des structures de donnÃ©es immuables dans des contextes modernes (concurrence, historique, etc.).

Une structure persistante ne veut pas dire â€œqui reste longtempsâ€, mais plutÃ´t : chaque version est conservÃ©e.

On ne modifie jamais la structure en place.
On crÃ©e une nouvelle version en rÃ©utilisant les parties communes (partage mÃ©moire).

Pourquoi faire Ã§a ?
- ParallÃ©lisme : plusieurs threads peuvent lire des structures sans se bloquer.
- DÃ©bogage : on peut revenir en arriÃ¨re.
- Undo/Redo : naturel Ã  implÃ©menter.
- TrÃ¨s utilisÃ© dans :
- Les Ã©diteurs de texte (type VS Code)
- Les frameworks fonctionnels (comme en Clojure, Elm, etc.)
- Les blockchains (Ã©tats immuables des blocs)

Message fort : immuabilitÃ© = cohÃ©rence + sÃ©curitÃ© + simplicitÃ© du code concurrent.

-->

---

# Big Data - Structures DistribuÃ©es

## MapReduce - Un Pattern Structural:
- **Map:** Transformer les donnÃ©es localement
- **Shuffle:** Redistribuer par clÃ©
- **Reduce:** AgrÃ©ger les rÃ©sultats

<br>

## Applications:
- **Google:** Indexation du web
- **Netflix:** Recommandations personnalisÃ©es
- **Banques:** DÃ©tection de fraudes
- **Sciences:** Analyse de gÃ©nomes

<br>

ğŸ’¡ **Insight:** Structure de traitement adaptÃ©e Ã  la structure des donnÃ©es massives !

ğŸŒ **DonnÃ©es massives = structures et algorithmes distribuÃ©s naturellement**

<!--
ğŸ¯ Objectif du slide : Expliquer que face Ã  des volumes massifs de donnÃ©es, la structure doit aussi Ãªtre rÃ©partie.

Quand on parle de Big Data, la structure ne tient plus sur une seule machine.

Il faut repenser la structure de maniÃ¨re distribuÃ©e.

Le schÃ©ma classique est celui de MapReduce :
- Map : chaque machine traite ses donnÃ©es localement.
- Shuffle : on redistribue les rÃ©sultats (par clÃ©).
- Reduce : on agrÃ¨ge les donnÃ©es.

Ce modÃ¨le est simpleâ€¦ mais structurellement puissant.

- Il a permis Ã  Google dâ€™indexer le web
- Ã€ Netflix dâ€™entraÃ®ner ses modÃ¨les de recommandation
- Aux chercheurs dâ€™analyser des gÃ©nomes entiers

Le point clÃ© : les algorithmes doivent sâ€™adapter Ã  la structure distribuÃ©e des donnÃ©es, sinon Ã§a ne scale pas.

Message fort : avec les donnÃ©es massives, la structure logique et la structure physique doivent Ã©voluer ensemble.

-->
